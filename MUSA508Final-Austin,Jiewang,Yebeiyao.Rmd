---
title: "Final project-Heroin Overdose Response"
author: "Austin Wheat, Jie Wang, Yebei Yao"
date: "12/10/2021"
output:
  html_document:
    highlight: zenburn
    theme: flatly
    toc: true
    toc_float: true
    code_folding: hide
---
# Youtube Link

The recording of our proposal is available on [Youtube](https://youtu.be/L1_NY9VaDBA). 

# Motivation

We are the team-Jay, Austin, Yebei and Jie are our group member. For our final project, we are doing the project option 1 which is predict herion overdoses events to better allocate prevention resources. So we regard ourselves as the geospatial analyst team in Mesa, and in order to respond the herion overdoses traveling throughout the city.We are going to make a GIS model to help predict areas in the future that may might happen herion overdoses. So we wanna the city of Mesa to fund and help to better protect the health of citizens throughout the city.  And with the current statistics of overdose condition in Meas, we have been known that within the last four years there has been approximately 11281 deaths relate to opiod. And in terms of the whole Maricopa County, in 2019, the total drug overdose deaths more than 1000, so we can see a very severe drug overdose situation is happening in the city of Mesa. And with the passing of the recent infrastructure bill by Biden government, we will be able to get some funds across the department like tranportation, health and able to help us continue to develop our model, and eventually an useful application/dashboard for the city to deploy and relocate resources to areas that are over overseen or over forgetten to the heroin overdoses. 
 
Our analysis is inspired by predictive policing, and using the the geospatial risk modelling. Throughout previous month, our team members developed successful crime prediction model for the city of Chicago. Which inspired us a lot for doing the heroin overdose in Mesa. The goal for a kind of model like overdose prediction or crime overdose is to borrow the experience from places where crime is observer, clustered and test whether that experience generalizes to places that may be also at risk, even though only few events of overdoses are reported.Predictions from geospatial risk models can be thought of as latent risk - areas at risk even if a overdoses has not actually been reported there. And with our model and analysis, the city of Mesa could easily replicate our analysis by importing the overdose incidents, to help predict the places that might has onging overdoses happening, and better allocate resources like amublance, or reallocate emergency resources near the overdoses to better respond the indicent. Some of the use cases while numerous we can apply, the two most important ones that we can think of, one is emergency response. In this case, our model can find areas that are not currently covered by a reasonable drive time are by ambulance and fire department's fire trucks, and then helping them delpoy to areas or roam around, or place new stations in those area where overdoses happen but not sufficiently covered. The second one is naloxone, in the state of Arizona naloxone can be received over the counter, we can partner either with local pharmacies and drug stores to plan place new locations in overdoses clutser area, or place vending machines with naloxone in these area can help to curb the issue and save lives.

And how will the stakeholders consume our model to better use it? The model and application we develop will be consumed quartely via a mobile application that generally for the normal citizen to aware the potential overdoses happen around them and a more complex dashboard for the government to manipulate and surveillance the latent risk.And our model is not only applicable for the herion overdoses, once our model perfroms well on prediction,Mesa could plugin anydata like burglary, theft, or other drug overdoses, to better protect and maintain the public safety and health. Some of the use cases while numerous we can apply, the two

# Data we choose

Data that employed in our model up to this point partially from the city itself, you can get it on the Mesa's Open Data Portal through this link:https://data.mesaaz.gov/ .  We pulled data from the Mesa Open Data Portal like crimes incidents, for crime incidents we are focousing on the drime that related to the drug for example drug paraphernalia possess-use, narcotic drug possess-use. Mostly the crimes that related to illegal drug possess and use. We also pulled data like streetlights locations that we assume high correlated to the drug uses and overdoses. We think the drug users would like to overdose under dark or shadow circumstance. 

We also pulled some data from an outside company called Safegraph，which collects point of interest data throughout the nation, and particularly for the city of Mesa we chose hospitals, bars, liquor stores and rehab centers. So we want to see the correlations between overdose incidents with this features. Locations like bars and liquor retail stroes that directly related to the drug overdoses, we will run the nearest neighbor analysis for these features to hypothesize a smoother exposure relationship across space.



**01.Introduction**
========================================

## 1.1 set up

To start up, we loaded the necessary library and functions for further analysis, as well as standardized format and color of maps and plots, the function.R script and delete the cross validation function, and reload it to remove the function error.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE)

library(tidyverse)
library(sf)
#install.packages('RSocrata')
library(RSocrata)
library(viridis)
library(spatstat)
library(raster)
library(spdep)
library(FNN)
library(grid)
library(gridExtra)
library(knitr)
library(kableExtra)
library(tidycensus)
library(tidyr)
library(dplyr)
library(magrittr)
library(mapview)
library(caret)
library(ckanr) 
library(ggcorrplot)
library(jtools)     
library(stargazer) 
library(broom)
library(tufte)
library(rmarkdown)
library(viridis)
library(spatstat) 
library(rgdal)


# functions
root.dir = "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/"
source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

remove(crossValidate)
crossValidate <-  function(dataset, id, dependentVariable, indVariables) {
  
  allPredictions <- data.frame()
  cvID_list <- unique(dataset[[id]])
  
  for (i in cvID_list) {
    
    thisFold <- i
    #cat("This hold out fold is", thisFold, "\n")
    
    fold.train <- filter(dataset, dataset[[id]] != thisFold) %>% as.data.frame() %>% 
      dplyr::select(id, geometry, indVariables, dependentVariable)
    fold.test  <- filter(dataset, dataset[[id]] == thisFold) %>% as.data.frame() %>% 
      dplyr::select(id, geometry, indVariables, dependentVariable)
    
    regression <-
      glm(paste0(dependentVariable,"~."), family = "poisson", 
          data = fold.train %>% 
            dplyr::select(-geometry, -id))
    
    thisPrediction <- 
      mutate(fold.test, Prediction = predict(regression, fold.test, type = "response"))
    
    allPredictions <-
      rbind(allPredictions, thisPrediction)
    
  }
  return(st_sf(allPredictions))
}

```

**02.Data wrangling**
========================================
## 2.1 Read in Data from Mesa city
### 2.1.1 base data and risk features

For this step, we load the data we pulled including boundary, and neighborhood. Unfortunately, the city of Mesa doesn't have any neighborhood, so we are using the census tracts as the neighborhood layer. Then we have the risk factors including liquor store, bar, hosptials, rehbab center, street lights, and crime incidents. For the crime incident, like we said above, we filter all the crimes that related to drug possess and use.  

```{r loading_data, message = FALSE, warning = FALSE}
#y 
setwd("E:\\Class\\MUSA508 public policy analytics\\Final\\MUSA-508-2021-Final-JAY")

# j
#setwd("D:/MUSA 508/MUSA-508-2021-Final-JAY")

#loading csv file
liquor <-st_read('data/Mesa_Liquor.csv')
Bar <- st_read('data/Mesa_Bars.csv')
Hospitals <- st_read('data/Mesa_Hospitals.csv')
Rehab <- st_read('data/Mesa_Rehab.csv')

#base data
##boundary:
Boundary <-st_read('https://data.mesaaz.gov/resource/qwhq-nske.geojson',crs='ESRI:102249')
##neighborhood(in census tract):
Neighborhood <-st_read('data/Mesa Census Tracts To City Boundary.geojson',crs='ESRI:102249')

  
##street center line:
Street <- st_read('https://data.mesaaz.gov/resource/9uib-89q3.geojson',crs='ESRI:102249')
##street light:
Stlight <- st_read('https://data.mesaaz.gov/resource/jrtd-htue.geojson',crs='ESRI:102249')


#mapview(Stlight)
```

### 2.1.2 Original overdose data and split into train/test sets

For this step, we split the original opioid data into test and train in time scale. The orginal overdose time range from 2017 to 2021. We set all the overdoses happened in 2019 as the test set, there are 180 overdose observations. For train set, we have remaining 820 overdose observations.We will use the observations in 2019 for further generalizability test.

```{r overdose,message = FALSE, warning = FALSE}
#overdose data
heroin_overdose_original <- st_read('https://data.mesaaz.gov/resource/qufy-tzv6.geojson',crs='ESRI:102249')

test_overdose<- filter(heroin_overdose_original,year==2019) ##180 observations
train_overdose<- filter(heroin_overdose_original,year!=2019)  ##820 observations

```

### 2.1.3 Fishnet and features wrangling

For this step, first we are creating the fishnet.Like what we did in the preditive policing, in this case, we are considering overdose risk not as a phenomenon that varies across boundary and municipalities units, but one varying smoothly across the landscape, like elevation. Imagine that overdose clusters in space, and that overdose risk dissipates outward from these ‘hot spots’, like elevation dips from mountaintops to valleys. The best way to represent this spatial trend in a regression-ready form, is to aggregate point-level data into a lattice of grid cells. And this grid cell is called fishnet.

```{r process_data, message = FALSE, warning = FALSE}
#fishnet
fishnet <- 
  st_make_grid(Boundary,
               cellsize = 0.005, 
               square = TRUE) %>%
  .[Boundary] %>%            # <- MDH Added
  st_sf() %>%
  mutate(uniqueID = rownames(.))

#point data
Stlight.sf <- Stlight%>%
  dplyr::select(geometry)%>%
  distinct()%>%
  mutate(Legend = "Stlight")


Bar.sf <- Bar%>%
  st_as_sf(coords = c("LONGITUDE", "LATITUDE"), crs = 'ESRI:102249', agr = "constant")%>%
  dplyr::select(geometry)%>%
  na.omit()%>%
  distinct()%>%
  mutate(Legend = "Bar")
  
liquor.sf <-liquor%>%
  st_as_sf(coords = c("LONGITUDE", "LATITUDE"), crs = 'ESRI:102249', agr = "constant")%>%
  dplyr::select(geometry)%>%
  na.omit()%>%
  distinct()%>%
  mutate(Legend = "Liquor")

Hospitals.sf <- Hospitals%>%
  st_as_sf(coords = c("LONGITUDE", "LATITUDE"), crs = 'ESRI:102249', agr = "constant")%>%
  dplyr::select(geometry)%>%
  na.omit()%>%
  distinct()%>%
  mutate(Legend = "Hospital")



Rehab.sf<- Rehab%>%
st_as_sf(coords = c("LONGITUDE", "LATITUDE"), crs = 'ESRI:102249', agr = "constant")%>%
  dplyr::select(geometry)%>%
  na.omit()%>%
  distinct()%>%
  mutate(Legend = "Rehab")

#Select all the crime type that relate to drug/drug
Police_incidents <-st_read('data\\Police_Incidents.csv',crs='ESRI:102249')%>%
   filter(grepl('DRUG',Crime.Type))%>%
    mutate(x = gsub("[()]", "", Location.1)) %>%
    separate(x,into= c("Y","X"), sep=",") %>%
    mutate(X = as.numeric(X),Y = as.numeric(Y)) %>% 
    na.omit() %>%
    st_as_sf(coords = c("X", "Y"), crs = 'ESRI:102249', agr = "constant")
    

Drugcrime.sf<-Police_incidents%>% 
  dplyr::select(geometry)%>%
  na.omit()%>%
  distinct()%>%
  mutate(Legend='Drug_Crime')

```

### 2.1.3 Visualizing Overall overdose tendency

For this section, the two maps below shows the heroin overdoses from 2017 to 201 across the city boundary of Mesa, and the map on the right shows the density of heroin overdose. From the two map, we can roughly know that most heroin overdoses happened and clustered at the city east, there is a big hotspot at the center city. And the Phoenix-Mesa Gateway Airport located on the south part of the city shows very small density.

```{r plotpointdata,message = FALSE, warning = FALSE}
#Plotting point data and density
grid.arrange(ncol=2,nrow=2,heights=(c(5,1)),widths=(c(1,1)),
ggplot() + 
  geom_sf(data = Boundary) +
  geom_sf(data =heroin_overdose_original, colour="#7a0177", size=1.5, show.legend = "point") +
  labs(title= "heroin overdose, Mesa - 2017-2021") +
  mapTheme(title_size = 12),

ggplot() + 
  geom_sf(data = Boundary) +
  stat_density2d(data = data.frame(st_coordinates(heroin_overdose_original)), 
                 aes(X, Y, fill = ..level.., alpha = ..level..),
                 size = 0.01, bins = 50, geom = 'polygon') +
  scale_fill_viridis(option = "plasma") +
  scale_alpha(range = c(0.00, 0.70), guide = FALSE) +
  labs(title = "Density of Heroin Overdose") +
  mapTheme(title_size = 12) + theme(legend.position = "none"))
```

### 2.2 Joining Overdose data to the Fishnet

In this section, we are joining the overdoses to the fishnet. And create overdose net with only train data set.
Take the fishnet grid as basis, we can get the count of assaults in each grid cell,spatial join the count with the fishnet,then get the sum of count in each cell. The grid cell with no assaults recieved NA is converted to 0,The unique ID and CvID is created for assigning name for each grid cell and to allow the 100-fold cross validation conducted below. The overall fishnet is 1640, we create CVID by calculting (1640/16≈100), in this way, we can randomly assign 100 folders for further testification.

```{r overdosenet,message = FALSE, warning = FALSE}
overdose_net <- 
  dplyr::select(train_overdose) %>% 
  mutate(countoverdose = 1) %>% 
  aggregate(., fishnet, sum) %>%
  mutate(countoverdose = replace_na(countoverdose, 0),
         uniqueID = rownames(.),
         cvID = sample(round(nrow(fishnet) / 16), size=nrow(fishnet), replace = TRUE))

ggplot() +
  geom_sf(data = Boundary) +
  geom_sf(data = overdose_net, aes(fill = countoverdose)) +
  scale_fill_viridis(option = "plasma") +
  labs(title = "Count of Heroin Overdose for the Fishnet") +
  mapTheme()
```

**03.Feature engineering**
========================================
## 3.1 Count of risk factors by grid cell

In this section, We start by joining a long form layer of overdose events to the vars_net fishnet. First, the individual risk factor layers are bound. Next, the fishnet is spatially joined to each point. The outcome is a large point data frame with a column for each fishnet uniqueID.

And then the map below shows each features as a small mulitple map shows the six risk factors by fishnet. These risk factors illustrate slightly different spatial processes. We could see for example, the drug related crime are also highly concentrated on the east side of the city, and very few around west side and south side which is the airpot. The streetlights also shows the same pattern.

```{r riskfactorcell,message = FALSE, warning = FALSE}
# Count of risk factors by grid cell
vars_net <- 
  rbind(Stlight.sf,Rehab.sf,Hospitals.sf,liquor.sf,Bar.sf,Drugcrime.sf) %>%
  st_join(., fishnet, join=st_within) %>%
  st_drop_geometry() %>%
  group_by(uniqueID, Legend) %>%
  summarize(count = n()) %>%
    full_join(fishnet) %>%
    spread(Legend, count, fill=0) %>%
    st_sf() %>%
    dplyr::select(-`<NA>`) %>%
    na.omit() %>%
    ungroup()

# transfer data into long format
vars_net.long <- 
  gather(vars_net, Variable, value, -geometry, -uniqueID)

# plot maps for each feature
vars <- unique(vars_net.long$Variable)
mapList <- list()

for(i in vars){
  mapList[[i]] <- 
    ggplot() +
      geom_sf(data = filter(vars_net.long, Variable == i), aes(fill=value), colour=NA) +
      scale_fill_viridis(option = "plasma",name="") +
      labs(title=i) +
      mapTheme()}

do.call(grid.arrange,c(mapList, ncol=3, top="Risk Factors by Fishnet"))

```

## 3.2 Nearest neighbor features in fishnet

For this part, since the grid cell impose a very rigid spatial scale of exposure, we added the second approach, to calculate average nearest neighbor distance to hypothesize a smoother exposure relationship across space.For this part. Here, the nn_function is used.Average nearest neighbor features are created by converting vars_net grid cells to centroid points then measuring to k risk factor points. And we set the k to 3.

And then the nearest neighbor features map are then mapped below. And generally speaking, we can see all the south part of the city which Mesa airport located has the highest nearest neighbor distance acroos all six risk factor features.

```{r nnneighbor,message = FALSE, warning = FALSE}
# Nearest neighbor features

st_c <- st_coordinates
st_coid <- st_centroid


vars_net <-
  vars_net %>%
    mutate(
      Hospital.nn =
        nn_function(st_c(st_coid(vars_net)), st_c(Hospitals.sf),3),
      Stlight.nn =
        nn_function(st_c(st_coid(vars_net)), st_c(Stlight.sf),3),
      Rehab.nn =
        nn_function(st_c(st_coid(vars_net)), st_c(Rehab.sf),3),
      liquor.nn =
        nn_function(st_c(st_coid(vars_net)), st_c(liquor.sf),3),
      Drugcrime.nn =
        nn_function(st_c(st_coid(vars_net)), st_c(Drugcrime.sf),3),
      Bar.nn =
        nn_function(st_c(st_coid(vars_net)), st_c(Bar.sf),3))



vars_net.long.nn <- 
  dplyr::select(vars_net, ends_with(".nn")) %>%
    gather(Variable, value, -geometry)

vars <- unique(vars_net.long.nn$Variable)
mapList <- list()

for(i in vars){
  mapList[[i]] <- 
    ggplot() +
      geom_sf(data = filter(vars_net.long.nn, Variable == i), aes(fill=value), colour=NA) +
      scale_fill_viridis(option = "plasma",name="") +
      labs(title=i) +
      mapTheme()}

do.call(grid.arrange,c(mapList, ncol = 3, top = "Nearest Neighbor risk Factors by Fishnet"))


```

## 3.3  Create the final_net

For this step, we join the assault net and variable net spatially, and using grid cell centroids(1.2.3), we spatially joined the neighborhood name and police district, and remove the grid cell centroids that do not fall into the neighborhood, the small neighborhood can be represented by one grid cell.

```{r finalnet,message = FALSE, warning = FALSE}

final_net <-
  left_join(overdose_net, st_drop_geometry(vars_net), by="uniqueID") 


final_net <-
  st_centroid(final_net) %>%
    st_join(dplyr::select(Neighborhood, name,geoid_data)) %>%
      st_drop_geometry() %>%
      left_join(dplyr::select(final_net, geometry, uniqueID)) %>%
      st_sf() %>%
  na.omit()

##mapview::mapview(final_net, zcol = "Rehab")
```


**04.Spatial Process Exploring Analysis**
========================================
## 4.1 Local Moran's I test and spatial process analysis

In this section, we will use a statistic called Local Moran's I. And the null hypothesis is that the overdose count at a given location is randomly distributed relative to its immediate neighbors.

For this step,a nearest neighbor weights matrix was used. Here, weights are calculated with ‘polygon adjacency’. And creates a neighbor list called ploy2nb,  and a spatial weights matrix, final_net.weights using queen contiguity. This means that every grid cell is related to its eight adjacent neighbors 
```{r weightmatrix,message = FALSE, warning = FALSE}
## {spdep} to make polygon to neighborhoods... 
final_net.nb <- poly2nb(as_Spatial(final_net), queen=TRUE)
## ... and neighborhoods to list of weigths
final_net.weights <- nb2listw(final_net.nb, style="W", zero.policy=TRUE)
#print(final_net.weights, zero.policy=TRUE)
```


For this step, we are calculating the Local Moran's I. Local Moran's I is a local spatial autocorrelation statistic that identifies local clusters or local outliers to understand their contribution to the 'global' clustering statistic. And have four outputs including I, the p-value, and Significiant_Hotspots, defined as those grid cells with higher local counts than what might otherwise be expected under randomness (p-values <= 0.05). 

```{r localmoranI,message = FALSE, warning = FALSE}
local_morans <- localmoran(final_net$countoverdose, final_net.weights, zero.policy=TRUE) %>% 
  as.data.frame()

# join local Moran's I results to finalnet
final_net.localMorans <- 
  cbind(local_morans, as.data.frame(final_net)) %>% 
  st_sf() %>%
  dplyr::select(countoverdose, 
                Local_Morans_I = Ii, 
                P_Value = `Pr(z != E(Ii))`) %>%
  mutate(Significant_Hotspots = ifelse(P_Value <= 0.001, 1, 0)) %>%
  gather(Variable, Value, -geometry)
  

```

This mulitple small maps illsutrate the Local Moran's I map, the local Moran's I and significant hotspots map ,the p-value map. And we can see two hotspots correlate to the density of overdoses on the east and mid.

```{r localmoranIplot,message = FALSE, warning = FALSE}
## This is just for plotting
vars <- unique(final_net.localMorans$Variable)
varList <- list()

for(i in vars){
  varList[[i]] <- 
    ggplot() +
      geom_sf(data = filter(final_net.localMorans, Variable == i), 
              aes(fill = Value), colour=NA) +
      scale_fill_viridis(option = "plasma",name="") +
      labs(title=i) +
      mapTheme(title_size = 14) + theme(legend.position="bottom")}


title=textGrob("Local Morans I statistics,overdose",gp=gpar(fontface="bold",fontsize=20))
grid.arrange(grobs=varList,ncol=2,top=title)
```

## 4.2 Distance to significant Hot spot

As before, dummy variable,"overdose.isSig" denotes a cell as part of a significant cluster (a p-value <= 0.0000001). "overdose.isSig.dist" then measures average nearest neighbor distance from each cell centroid to its nearest significant cluster.

The figure below shows average nearest neighbor distance from each cell centroid to its nearest significant cluster. We can now model important information on the local spatial process ofoverdose.

```{r distancenndis,message = FALSE, warning = FALSE}
# Distance to highly significant hotpot
final_net <- final_net %>% 
  mutate(overdose.isSig = 
           ifelse(local_morans[,5] <= 0.001, 1, 0)) %>%
  mutate(overdose.isSig.dist = 
           nn_function(st_c(st_coid(final_net)),
                       st_c(st_coid(filter(final_net, 
                                           overdose.isSig == 1))), 
                       k = 1))

ggplot() +
      geom_sf(data = final_net, aes(fill=overdose.isSig.dist), colour="#807dba") +
      scale_fill_viridis(option = "plasma",name="NN Distance") +
      labs(title="Distance to Highly Significant Overdose Hotspot") +
      mapTheme()
```


## 4.3 Correlation tests

Now we explore the correlation between the each risk factors and dependent variable, assault incidents.With a small multiple scatterplot of countoverdose as a function of the risk factors, it is clear that most features have shown controversial trends between itself and the nearest neighbor feature, which in a way can help further feature selection in modeling.

```{r  correlation, fig.width=9, fig.height=12,message = FALSE, warning = FALSE}
correlation.long <-
  st_drop_geometry(final_net) %>%
    dplyr::select(-uniqueID, -cvID, -name, -geoid_data) %>%
    gather(Variable, Value, -countoverdose)

correlation.cor <-
  correlation.long %>%
    group_by(Variable) %>%
    summarize(correlation = cor(Value, countoverdose, use = "complete.obs"))

ggplot(correlation.long, aes(Value, countoverdose)) +
  geom_point(size = 1,color="#7a0177") +
  geom_text(data = correlation.cor, 
            aes(label = paste("r =", round(correlation, 2))),
            x=-Inf, y=Inf, vjust = 1.5, hjust = -1) +
  geom_smooth(method = "lm", se = FALSE, colour = "black") +
  facet_wrap(~Variable, ncol = 2, scales = "free") +
  labs(title = "Heroin Overdose as a function of risk factors") +
  plotTheme()
```


## 4.4 A histogram of dependent variable-Overdose

Learning from the histogram of count of overdose cases, the distribution is skewed,it is reasonable because most overdose incidents just happened without reported, which is a kind of selection bias. As consequence,it is likely that most grid cells contain no overdose incidents.Based on the histogram below, the distribution of the dependent variable is Poisson distribution.

In conclusion,the approach of Poisson Regression is introduced,which uniquely suited to modeling a count outcome like overdose incidents in this case.

```{r distancenn,message = FALSE, warning = FALSE}
ggplot(data = final_net) +
  geom_histogram(aes(x = countoverdose), fill = '#d0d1e6',color="#0570b0") +
  scale_x_continuous(breaks = seq(0, 8, by = 1)) + 
  labs(title="Histogram of Dependent Variable: Count of Heroin Overduse",
       subtitle = "Mesa, AZ\n") +
  xlab('Count of Overdose') +
  ylab('Count') +
  plotTheme()


```



**05.Poisson Regression**
========================================
## 5.1 Model Building and Cross-Validation

In this part, we are going to do the poission regression model.A Poisson Regression is estimated which is uniquely suited to modeling a count outcome like count of overdose. Poisson regression is often used for modeling count data, and it has a number of extensions useful for count models. It means the response variable is a count per unit of time or space, described by a Poisson distribution.

## 5.1.1 Model Building

Because "final_net" is not split into training and test sets, and because geospatial risk models are purely spatial,additionally, since that these are also places where people are just using heroin without being caught, we directly move to spatial cross-validation,which tests whether model is generalized or not.
According to correlation plots,map of Distance to Highly Significant Assault Hotspot,so as to avoid collinerity in the model as much as possible.Then I select the necessary predictors and create two variable list, the one named "reg.vars" contains just risk factors, and another one "reg.ss.vars" contains both risk factors and the spatial process features created before, in section 4.2,"Distance to significant Hot spot".

```{r vars,message = FALSE, warning = FALSE}

reg.vars <- c('Hospital.nn','Stlight.nn','Rehab.nn','liquor.nn','Drugcrime.nn','Bar.nn')

reg.ss.vars <- c('Hospital.nn','Stlight.nn','Rehab.nn','liquor.nn','Drugcrime.nn','Bar.nn', 'overdose.isSig', 'overdose.isSig.dist')
```

## 5.1.1 Cross validation.

In this section,with the two variable sets,and two kinds of group units (random CV IDs, neighborhood names), we create four cross validations to test the generalizability of the model.The cross validation models are mainly two kinds, the first one is random k-fold cross validation,k is created before when creating "assault_net", the total fishnet is 2458, and CVID is created for around (2458/24≈100) fold, in this case, the fishnets are randomly assigned into different 100 folds.
The next kinds is called "LOGO-CV", which means "Leave-One-Group_Out",that is, to hold out one local area, train the model on the remaining n - 1 areas, predict for the hold out, and record the goodness of fit, repeat again and again until each neighborhood takes a turn as a hold-out.

The cross-validation model with "reg.vars" are the ones includes Just Risk Factors, the cross-validation model with "reg.ss.vars" are the ones includes both risk factors and the Local Moran’s I Spatial Process features.

```{r cv,message = FALSE, warning = FALSE}

#View(crossValidate)

reg.cv <- crossValidate(
  dataset =final_net,
  id = "cvID",
  dependentVariable = "countoverdose",
  indVariables = reg.vars) %>%
    dplyr::select(cvID = cvID, countoverdose, Prediction, geometry)

reg.ss.cv <- crossValidate(
  dataset = final_net,
  id = "cvID",
  dependentVariable = "countoverdose",
  indVariables = reg.ss.vars) %>%
    dplyr::select(cvID = cvID, countoverdose, Prediction, geometry)
  
reg.spatialCV <- crossValidate(
  dataset = final_net,
  id = "name",
  dependentVariable = "countoverdose",
  indVariables = reg.vars) %>%
    dplyr::select(cvID = name, countoverdose, Prediction, geometry)

reg.ss.spatialCV <- crossValidate(
  dataset = final_net,
  id = "name",
  dependentVariable = "countoverdose",
  indVariables = reg.ss.vars) %>%
    dplyr::select(cvID = name, countoverdose, Prediction, geometry)
```

## 5.2 Accuracy & Generalizability

### 5.2.1 mean and standard deviation of MAE by regression

We calculate a host of goodness of fit metrics with particular emphasis on generalizability across space. The code block below creates a long form summary data frame, that binds together observed/predicted counts and errors for each grid cell and for each Regression, along with the 'cvID', and 'the geometry'.

```{r goodnessfitmetrics,message = FALSE, warning = FALSE}
reg.summary <- 
  rbind(
    mutate(reg.cv,           Error = Prediction -countoverdose,
                             Regression = "Random k-fold CV: Just Risk Factors"),
                             
    mutate(reg.ss.cv,        Error = Prediction - countoverdose,
                             Regression = "Random k-fold CV: Spatial Process"),
    
    mutate(reg.spatialCV,    Error = Prediction - countoverdose,
                             Regression = "Spatial LOGO-CV: Just Risk Factors"),
                             
    mutate(reg.ss.spatialCV, Error = Prediction - countoverdose,
                             Regression = "Spatial LOGO-CV: Spatial Process")) %>%
    st_sf() 
```

### 5.2.2 Distribution of MAE

In the plot below shows the MAE for both K-fold and LOGO-CV, and one is just risk factors, one added the sptial process features are added. We can see that when the local spatial process is not accounted for, there MAEs are very close to 3, when the spatial process features are added, the low MAEs amount is increasing, which indicate that spatial process could improve our model accuracy. 

```{r MAE,message = FALSE, warning = FALSE}
error_by_reg_and_fold <- 
  reg.summary %>%
    group_by(Regression, cvID) %>% 
    summarize(Mean_Error = mean(Prediction - countoverdose, na.rm = T),
              MAE = mean(abs(Mean_Error), na.rm = T),
              SD_MAE = mean(abs(Mean_Error), na.rm = T)) %>%
  ungroup()

error_by_reg_and_fold %>%
  ggplot(aes(MAE)) + 
    geom_histogram(bins = 30,  fill = '#d0d1e6',color="#0570b0") +
    facet_wrap(~Regression) +  
    geom_vline(xintercept = 0) + 
  scale_x_continuous(breaks = seq(0, 8, by = 1)) + 
    labs(title="Distribution of MAE", subtitle = "k-fold cross validation vs. LOGO-CV",
         x="Mean Absolute Error", y="Count") +
    plotTheme()
```

The table below builds on error_by_reg_and_fold to calculate the mean and standard deviation in errors by regression.  The result confirms our conclusion that the Spatial Process features improve the model. The model appears slightly less robust for the spatial cross-validation because LOGO-CV is such a conservative assumption. 

```{r  sdmae, echo=TRUE, message=FALSE, warning=FALSE}
st_drop_geometry(error_by_reg_and_fold) %>%
  group_by(Regression) %>% 
    summarize(Mean_MAE = round(mean(MAE), 2),
              SD_MAE = round(sd(MAE), 2)) %>%
  kable(caption = "Table 1. MAE_SD by regression") %>%
    kable_styling("striped", full_width = F) %>%
    row_spec(2, color = "black", background = "#ece7f2") %>%
    row_spec(4, color = "black", background = "#ece7f2") 
```

### 5.2.3 LOGO-CV overdose errors spatial visualizing map

The figure below visualizes the LOGO-CV errors spatially. These maps visualize where the higher errors occur when the local spatial process is not accounted for. Not surprisingly, the largest errors are in the hotspot locations we keep talking above which are city east and mid part.

```{r kfold,echo=TRUE, message=FALSE, warning=FALSE}
error_by_reg_and_fold %>%
  filter(str_detect(Regression, "LOGO")) %>%
  ggplot() +
    geom_sf(aes(fill = MAE)) +
    facet_wrap(~Regression) +
    scale_fill_viridis(option = "plasma",name = " ") +
    labs(title = "Heroin overdose errors by LOGO-CV Regression") +
    mapTheme() + theme(strip.text.x = element_text(size=10),axis.text.x = element_text(angle = 45, vjust = 0.5))+
  plotTheme()
```

### 5.2.4 Moran's I on Erros By Regression

The below chunk calculates spatial weights matrix at the neighborhood scale instead of grid cell scale.With the new spatial weight matrix, the table below shows a new neighborhood.weights, spatial weights matrix at the neighborhood instead of grid cell scale. Global Moran’s I and p-values are then calculated for each LOGO-CV regression. This section also give more evidence that our spatial process features helped account for the spatial variation in overdoses, although some still remains.

```{r errorneigh,message = FALSE, warning = FALSE}
neighborhood.weights <-
  filter(error_by_reg_and_fold, Regression == "Spatial LOGO-CV: Spatial Process") %>%
    group_by(cvID) %>%
      poly2nb(., queen=TRUE) %>%
      nb2listw(., style="W", zero.policy=TRUE)

filter(error_by_reg_and_fold, str_detect(Regression, "LOGO"))  %>% 
    st_drop_geometry() %>%
    group_by(Regression) %>%
    summarize(Morans_I = moran.mc(abs(Mean_Error), neighborhood.weights, 
                                 nsim = 999, zero.policy = TRUE, 
                                 na.action=na.omit)[[1]],
              p_value = moran.mc(abs(Mean_Error), neighborhood.weights, 
                                 nsim = 999, zero.policy = TRUE, 
                                 na.action=na.omit)[[3]])%>%
    kable()%>%
  kable_styling("striped", full_width = F) %>%
    row_spec(1, color = "black", background = "#ece7f2")
    
```

### 5.2.5 Predicted overdose and observed overdose

The figure below shows the our prediction with observed overdose.  And our models shows the over prediction for the low overdoses decile area and under predict in hot spot areas. Over-predictions in lower overdose areas could highlight areas of latent risk. Under-prediction in higher overdose areas reflect difficulty predicting the hotspots.

```{r preandob,message = FALSE, warning = FALSE}
st_drop_geometry(reg.summary) %>%
  group_by(Regression) %>%
    mutate(overdose_Decile = ntile(countoverdose, 10)) %>%
  group_by(Regression, overdose_Decile) %>%
    summarize(meanObserved = mean(countoverdose, na.rm=T),
              meanPrediction = mean(Prediction, na.rm=T)) %>%
    gather(Variable, Value, -Regression, -overdose_Decile) %>%          
    ggplot(aes(overdose_Decile, Value, shape = Variable)) +
      geom_point(size = 2) + geom_path(aes(group = overdose_Decile), colour = "black") +
      scale_shape_manual(values = c(2, 17)) +
      facet_wrap(~Regression) + xlim(0,10) +
      labs(title = "Predicted and observed overdose by observed overdose decile")  +
      plotTheme()
```

## 5.4 Kernal Density-Comparison with the traditional model

### 5.4.1 Kernel density maps


In this section, we going to explore whether the poisson model allocate better than traditional overdose hotspots.To add an element of across-time generalizability, hotspot and risk predictions from other year overdose incidents are used to predict the location of overdose from 2019.

Kernel density works by centering a smooth kernel, or curve, atop each crime point such that the curve is at its highest directly over the point and the lowest at the range of a circular search radius. The density in a particular place is the sum of all the kernels that underlie it.Thus, areas with many nearby points have relatively high densities.The key scale assumption in kernel density is the use of a global search radius parameter. Because the creation of kernel density is based on nearby points,it shows clearly about spatial autocorrelation.

The code chunk below create three kernel density map by using 500, 800 and 1000 foot search radius.

```{r kerneldensity, echo=TRUE, message=FALSE, warning=FALSE}


overdose_ppp <- as.ppp(st_coordinates(train_overdose), W = st_bbox(final_net))

overdose_KD.500 <- spatstat.core::density.ppp(overdose_ppp, 500)
overdose_KD.800 <- spatstat.core::density.ppp(overdose_ppp, 800)
overdose_KD.1000 <- spatstat.core::density.ppp(overdose_ppp, 1000)


overdose_KD.df <- rbind(
  mutate(data.frame(rasterToPoints(mask(raster(overdose_KD.500), as(Neighborhood, 'Spatial')))), Legend = "500 Ft."),
  mutate(data.frame(rasterToPoints(mask(raster(overdose_KD.800), as(Neighborhood, 'Spatial')))), Legend = "800 Ft."),
  mutate(data.frame(rasterToPoints(mask(raster(overdose_KD.1000), as(Neighborhood, 'Spatial')))), Legend = "1000 Ft.")) 

overdose_KD.df$Legend <- factor(overdose_KD.df$Legend, levels = c("500 Ft.", "800 Ft.", "1000 Ft."))

ggplot(data=overdose_KD.df, aes(x=x, y=y)) +
  geom_raster(aes(fill=layer)) + 
  facet_wrap(~Legend) +
  coord_sf(crs=st_crs(final_net)) + 
  scale_fill_viridis(option = "plasma",
                       name = "Density") +
  labs(title = "Kernel Density with 3 Different Search Radius Scales") +
  mapTheme(title_size = 14)+
   theme(strip.text.x = element_text(size=10),axis.text.x = element_text(angle = 45, vjust = 0.5))+
  plotTheme()
```

This map shows the kernel density of overdose other than 2019.

```{r kernelmap,echo=TRUE, message=FALSE, warning=FALSE}
as.data.frame(overdose_KD.500) %>%
    st_as_sf(coords = c("x", "y"), crs = st_crs(final_net)) %>%
  aggregate(., final_net, mean) %>%
   ggplot() +
     geom_sf(aes(fill=value)) +
     geom_sf(data = (train_overdose),size = 1) +
     scale_fill_viridis(option = "plasma",name = "Density") +
     labs(title = "Kernel density of overdose incidents except for 2019") +
     mapTheme(title_size = 14)
```

### 5.4.2 goodness of fit in 2019 overdose mapping

Next, we want to to predict overdose in 2019 with hot spot approach, Kernel density is computed on the train overdose sets. converts the density to 100 deciles,and classified to 5 risk categories.For our kernel density, which shows that the highest risk category are located at the most west side of the city, and more to the east the more lower risk category level.


```{r predicttest,echo=TRUE, message=FALSE, warning=FALSE}
##bind test data in 2019 with kernel density map
r_ppp.19 <- as.ppp(st_coordinates(test_overdose), W = st_bbox(final_net))


r_KD.500.19 <- density.ppp(r_ppp.19, 500)
r_KD.800.19 <- density.ppp(r_ppp.19, 800)
r_KD.1000.19 <- density.ppp(r_ppp.19, 1000)


overdose_KDE_sf <- as.data.frame(r_KD.500.19) %>%
  st_as_sf(coords = c("x", "y"), crs = st_crs(final_net)) %>%
  aggregate(., final_net, mean) %>%
  mutate(label = "Kernel Density",
         Risk_Category = ntile(value, 100),
         Risk_Category = case_when(
           Risk_Category >= 90 ~ "90% to 100%",
           Risk_Category >= 70 & Risk_Category <= 89 ~ "70% to 89%",
           Risk_Category >= 50 & Risk_Category <= 69 ~ "50% to 69%",
           Risk_Category >= 30 & Risk_Category <= 49 ~ "30% to 49%",
           Risk_Category >= 1 & Risk_Category <= 29 ~ "1% to 29%")) %>%
  # count assaults in 2018 for each cell
  cbind(
    aggregate(
      dplyr::select(test_overdose) %>% mutate(countoverdose = 1), ., sum) %>%
    mutate(countoverdose = replace_na(countoverdose, 0))) %>%
  dplyr::select(label, Risk_Category, countoverdose)


## Prediction by Risk Prediction Model
overdose_risk_sf <-
  filter(reg.summary, Regression == "Random k-fold CV: Spatial Process") %>%
  mutate(label = "Risk Predictions",
         Risk_Category = ntile(Prediction, 100),
         Risk_Category = case_when(
           Risk_Category >= 90 ~ "90% to 100%",
           Risk_Category >= 70 & Risk_Category <= 89 ~ "70% to 89%",
           Risk_Category >= 50 & Risk_Category <= 69 ~ "50% to 69%",
           Risk_Category >= 30 & Risk_Category <= 49 ~ "30% to 49%",
           Risk_Category >= 1 & Risk_Category <= 29 ~ "1% to 29%")) %>%
  cbind(
     aggregate(
      dplyr::select(test_overdose) %>% mutate(countoverdose = 1), ., sum) %>%
    mutate(countoverdose = replace_na(countoverdose, 0))) %>%
  dplyr::select(label, Risk_Category, countoverdose)

##plot comparison
rbind(overdose_KDE_sf,overdose_risk_sf) %>%
  na.omit() %>%
  gather(Variable, Value, -label, -Risk_Category, -geometry) %>%
  ggplot() +
  geom_sf(aes(fill = Risk_Category), colour = NA) +
  geom_sf(data = test_overdose, size = .5, colour = "black") +
  facet_wrap(~label, ) +
  scale_fill_viridis_d(option = "plasma",
                       name = 'Risk Category') + 
  labs(title="Comparison of Kernel Density and Risk Predictions",
       subtitle="2019 overdose risk predictions vs Other years") +
  mapTheme() +
   theme(strip.text.x = element_text(size=10),axis.text.x = element_text(angle = 45, vjust = 0.5))+
  plotTheme()

```

### 5.4.3 Risk prediction and kernel density bar plot

Finally, the code block below calculates the rate of 2019 overdoses with others by risk category and model type. Our model show that the risk predictions capture a greater share of 2019 overdoses in the highest risk category relative to the Kernel density which indicate our model is well fit.

The risk prediction model narrowly edges out the Kernel Density in the top two highest risk categories, the two highest risk categories have much higher rate of overdoses than other three risk categories - suggesting this simple model has some value relative to the business-as-usual hot spot approach. It indicates that in more higher risk categories area, there will be more overdoses happening.

```{r barplot,echo=TRUE, message=FALSE, warning=FALSE}
rbind(overdose_risk_sf,overdose_KDE_sf ) %>%
  st_set_geometry(NULL) %>% 
  na.omit() %>%
  gather(Variable, Value, -label, -Risk_Category) %>%
  group_by(label, Risk_Category) %>%
  summarize(countoverdose= sum(Value)) %>%
  ungroup() %>%
  group_by(label) %>%
  mutate(Rate_of_test_set_crimes = countoverdose/ sum(countoverdose)) %>%
  ggplot(aes(Risk_Category,Rate_of_test_set_crimes)) +
  geom_bar(aes(fill=label), position="dodge", stat="identity") +
  scale_fill_manual(values=c("#a6bddb","#fed98e"))+
  labs(title = "Risk Prediction vs. Kernel Density, 2019 vs Other Years Overdose Incidents",
       x = 'Risk Category',
       y = 'Rate of Test Set Overdose Crimes') +
  plotTheme()+
  theme(axis.text.x = element_text(angle = 45, vjust = 0.5))
```

**06.Can ambulance reach the incidents in time?**
========================================
In this section, we brought our model into a widely used case which is ambulance resource allocation. Currently, when hospital or rescue center receive the emergency calls that there is someone caught overdose drugs in some place and is in need of help,they will assign the nearest ambulance vehicle around the spot, which will cause some part of city with relatively lower accessibility with the ambulance resources.With the predicted overdose incidents, combing it with the hospital resources,we can know which part of the city has higher risk of overdose but lower accessibility to get in time help, and maximize the overall expected survival probability of overdoses.

## 6.1 isochrones map in 8 minutes 
An isochrone map in geography and urban planning is a map that depicts the area accessible from a point within a certain time threshold
With the help of QGIS, we are able to create the driving travel time threshold from each hospitals around the city.
Consider the ambulance will go back and forth before the drugger really get help, and there are also some time needed like congestion, transferring or physical check,,etc,. We assume it will be better if we constrain the time limitation within thirty minutes before the drugger get treatment.
So we create the eight-minute driving travel time threshold taking ach hospitals as the central point, so the average time consuming on commuting will be 16 minutes, and the druggers will have high possibility to get timely help.

```{r drivingthreshold,echo=TRUE, message=FALSE, warning=FALSE}
Eight_Min_driving <-st_read('data/HospitalSheds8.geojson',crs='ESRI:102249')%>%
  mutate(ID = rownames(.))
```

## 6.2 Combine the final_net with isochrones

By aggregate the predicted kernel density map with overall overdose incidents and the eight-minute driving travel time threshold,for further illustration, we map all the overdose incidents happened in the past five years,in this way, we are able to find the hotspot space of overdose incidents, and the safe zone that locate near from the hospitals.On the other hand, where there have higher risk of overdose incidents(in this case the yellow district) but locates outside the time threshold,which means druggers in this space can't get in time help, there should be more treatment or ambulance station, like contemporary treatment center of ambulance parking lots. Learning from the map below, some yellow space especially western city does have lower accessibility of timely medical treatment.

```{r ambulance,echo=TRUE, message=FALSE, warning=FALSE}
final_net_time <-
  st_centroid(final_net) %>%
    st_join(dplyr::select(Eight_Min_driving,ID)) %>%
      st_drop_geometry() %>%
      left_join(dplyr::select(final_net, geometry, uniqueID)) %>%
      st_sf() %>%
  na.omit()


as.data.frame(overdose_KD.500) %>%
    st_as_sf(coords = c("x", "y"), crs = st_crs(final_net_time)) %>%
  aggregate(., final_net, mean) %>%
   ggplot() +
  geom_sf(data=Boundary,fill="transparent",color="black")+
     geom_sf(aes(fill=value)) +
     geom_sf(data = (heroin_overdose_original),size = 1) +
     scale_fill_viridis(option = "plasma",name = "Density") +
 geom_sf(data = Eight_Min_driving,color="#f7fbff",fill="#4292c6",alpha=0.3)+
     labs(title = "Kernel density of all overdose incidents",
          subtitle="highlightened space locate outside time threshold needs more treatment center!!") +
     mapTheme(title_size = 14)+
plotTheme()
```

**07.Conclusion**
========================================
To make a conclusion, we think our model plays a good job for predicting heroin overdoses. We are use the geospatial risk modelling to predict and test whether that experience generalizes to places where overdose risk may be high, despite few actual events. We introduced new and powerful feature engineering strategies like grocery stores, streetlights,liquor and bar to capture the local spatial process. Spatial cross-validation was also introduced as an important test of across-space generalizability. In order to test whether the model we build is generalized through time scope, we train our model with 820 observations in 2017 through 2021 except for 2019, and test it in time period of 2019, and the result shows it  have done a great job.

And for the heroin overdose in Mesa, we think the most cases are happened in east and center part of the city across time and space from 2017 to 2021. That high correlated to the drug relate crime, drug possess and uses case happened across the city.

Our model will be useful in many fields,the main proposed use cases from us is firstly the **ambulance resource allocation**, we use our model to predict the overall distrubution of heroin overdose, and then with the help of isochrone map, created the driving travel time threshold from each hospitals so as to learn the treatment accessibility around city, after comparison, it is clear that some place especially western city with higher hotspots of overdose incidens does have lower medical resource and should be taken into consideration.

The second one is to **Improve Naloxone Distribution**. Naloxone is a safe drug that can reverse opioid overdose, after we successfully predicted the potential district that may emerge more heroin overdose, the public health department can put more Naloxone in the nearby pharmacy like CVs, in this case, the druggers can easily get access to the Life-saving medicine for good.

To further illustrate, if we want to optimize our analysis better, we can take more considerations of selection bias,because the overdose in a kind of illegal crime, some people are just using heroin without being caught, so these part of data is hidden inside the black box.Also, the data online have some accuracy diminished to make them more anonymous in order for public use, which makes harder for us to avoid selection bias. 





